{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"9yRIhS6lOTYX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734292490900,"user_tz":300,"elapsed":19987,"user":{"displayName":"mia yu","userId":"18290570536680028134"}},"outputId":"26b2c03b-6fad-4056-fe99-87b9e426c895"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.5)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Collecting datasets\n","  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2024.10.0\n","    Uninstalling fsspec-2024.10.0:\n","      Successfully uninstalled fsspec-2024.10.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"]}],"source":["# !pip install datasets==2.10.1\n","#load_dataset sometimes hangs on a higher version\n","!pip install transformers\n","\n","!pip install -U datasets"]},{"cell_type":"markdown","metadata":{"id":"JifsBqXxmtqm"},"source":["# Preprocessing"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"OjX3Y-kCzdpE","executionInfo":{"status":"ok","timestamp":1734292503730,"user_tz":300,"elapsed":12834,"user":{"displayName":"mia yu","userId":"18290570536680028134"}}},"outputs":[],"source":["from datasets import load_dataset\n","\n","import torch\n","import numpy as np\n","import random\n","\n","# we set up some seeds so that we can reproduce results\n","seed = 123\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","\n","np.random.seed(seed)\n","random.seed(seed)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"gJOgVaQtwRgj","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1734292503730,"user_tz":300,"elapsed":5,"user":{"displayName":"mia yu","userId":"18290570536680028134"}},"outputId":"78d87e3a-5f5f-4986-d05d-4c40fba6f690"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nSome options for BERT model that can be run in colab:\\n\\n\"distilbert-base-uncased\",\\n\"distilbert-base-uncased-distilled-squad\",\\n\"distilbert-base-cased\",\\n\"distilbert-base-cased-distilled-squad\",\\n\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["\"\"\"\n","Some options for BERT model that can be run in colab:\n","\n","\"distilbert-base-uncased\",\n","\"distilbert-base-uncased-distilled-squad\",\n","\"distilbert-base-cased\",\n","\"distilbert-base-cased-distilled-squad\",\n","\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"VwJLAnpb742W"},"source":["# load data"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"eMoGcjRgMk1E","colab":{"base_uri":"https://localhost:8080/","height":99,"referenced_widgets":["77ed9a8bb0cb46f0bb46df98c91b2dca","d541d825f5d549d1b7d992b099348f42","a121333de8304e6b9acdaefcea87c46e","5d4b80a451a24a539afcd29e2e92a360","4f512deb6f224f86ab260ba2a71aface","0e2961d7476b43a8bbb901c3ceb3faa1","db074b91eb0b4256b05011c09d241d31","94c3a200d9394462990e9dbed3e78b4f","9a52eaa608004d4b81d907d25c35bb93","65e80179c64a441cb0fd8ae48c7add95","428050ca55d049d4b1a7ef2c52a49aaf","41ea3082066c47959cf3c83404bb4326","f489a1d89ee24e3d9e4cdb8ab11d18ac","d1cf3cb3f4b847889222dcc3104eaa3c","8d6a43d53f384e32bcc99aea31b92e79","d9a21354bcb648e4a7d7549d8c4a31ae","c6241b90c539417ab85ff6d7e079f0ee","c7d5eda9338546a292f7456d7d9a87cd","6abc01d93d4146ec8af1b71a642289c7","85fc44e76df942a59c76a65703439b92","06928d9c9fc7437897a5d8150579787d","8573b9a9e872493e9c5ebba9bc3fe4cd"]},"executionInfo":{"status":"ok","timestamp":1734292536789,"user_tz":300,"elapsed":33062,"user":{"displayName":"mia yu","userId":"18290570536680028134"}},"outputId":"ad123e46-ab8f-4ba7-bd43-cfd8770158a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77ed9a8bb0cb46f0bb46df98c91b2dca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating dev split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41ea3082066c47959cf3c83404bb4326"}},"metadata":{}}],"source":["# Change train.json / dev.json to the appropriate filepaths =====\n","from google.colab import drive\n","from datasets import load_dataset\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# File paths\n","FILEPATH = \"/content/drive/MyDrive/CSCI1460/final2/\"\n","\n","# Update data_files with correct paths\n","data_files = {\n","    \"train\": f\"{FILEPATH}all_train.json\",\n","    \"dev\": f\"{FILEPATH}all_dev.json\"\n","}\n","\n","\n","# data_files = {\"train\": \"/content/all_train.json\", \"dev\": \"/content/all_dev.json\"}\n","\n","# Load dataset\n","dataset = load_dataset('json', data_files=data_files)\n","\n","from google.colab import drive\n","from datasets import load_dataset\n","\n","def load_data(dataset):\n","    \"\"\"\n","    Load the dataset using Hugging Face datasets library.\n","\n","    Returns:\n","        train_data (list): List of training examples.\n","        val_data (list): List of validation examples.\n","    \"\"\"\n","    train_data = dataset[\"train\"]\n","    val_data = dataset[\"dev\"]\n","    return train_data, val_data\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"RCtyr-EGXdei","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734292540143,"user_tz":300,"elapsed":3358,"user":{"displayName":"mia yu","userId":"18290570536680028134"}},"outputId":"18633220-8157-424a-ebc9-d11f48f7fa2a"},"outputs":[{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['name', 'id', 'questions', 'answers', 'has_correct_context', 'contexts'],\n","        num_rows: 27866\n","    })\n","    dev: Dataset({\n","        features: ['name', 'id', 'questions', 'answers', 'has_correct_context', 'contexts'],\n","        num_rows: 1743\n","    })\n","})\n","{'name': 'Truman Doctrine', 'id': '5896064246644527845', 'questions': [{'input_text': 'what was the key goal of the truman doctrine'}], 'answers': [{'candidate_id': 0, 'input_text': 'short', 'span_end': 136, 'span_start': 76, 'span_text': 'to counter Soviet geopolitical expansion during the Cold War'}], 'has_correct_context': True, 'contexts': \"The Truman Doctrine was an American foreign policy whose stated purpose was to counter Soviet geopolitical expansion during the Cold War . It was first announced to Congress by President Harry S. Truman on March 12 , 1947 , and further developed on July 12 , 1948 , when he pledged to contain threats to Greece and Turkey . Direct American military force was usually not involved , but Congress appropriated financial aid to support the economies and militaries of Greece and Turkey . More generally , the Truman Doctrine implied American support for other nations allegedly threatened by Soviet communism . The Truman Doctrine became the foundation of American foreign policy , and led , in 1949 , to the formation of NATO , a military alliance that is still in effect . Historians often use Truman 's speech to date the start of the Cold War .\"}\n","Training dataset:\n","                                            name                    id  \\\n","0             Stephanie Edwards (Grey's Anatomy)   5495190773098085777   \n","1                                Truman Doctrine   5896064246644527845   \n","2                   2008 US Open – Mixed Doubles   -456240753909481859   \n","3  United States presidential line of succession  -1360639587714376154   \n","4                                Malcolm Winters   1480047205502539560   \n","\n","                                           questions  \\\n","0  [{'input_text': 'when does stephanie die in gr...   \n","1  [{'input_text': 'what was the key goal of the ...   \n","2  [{'input_text': 'who was leander paes partner ...   \n","3  [{'input_text': 'who takes over after a presid...   \n","4  [{'input_text': 'who plays malcolm winters on ...   \n","\n","                                             answers  has_correct_context  \\\n","0  [{'candidate_id': 0, 'input_text': 'short', 's...                 True   \n","1  [{'candidate_id': 0, 'input_text': 'short', 's...                 True   \n","2  [{'candidate_id': 0, 'input_text': 'short', 's...                 True   \n","3  [{'candidate_id': 0, 'input_text': 'short', 's...                 True   \n","4  [{'candidate_id': 0, 'input_text': 'short', 's...                 True   \n","\n","                                            contexts  \n","0  Dr. Stephanie Edwards Grey 's Anatomy characte...  \n","1  The Truman Doctrine was an American foreign po...  \n","2  Mixed Doubles 2008 US Open Champions Cara Blac...  \n","3  The United States presidential line of success...  \n","4  Malcolm Winters Shemar Moore as Malcolm Winter...  \n","\n","Development dataset:\n","                                        name                    id  \\\n","0                               Mandalay Bay   7811140318762480311   \n","1                    The Shannara Chronicles   5770553296217961046   \n","2  Taylor Hayes (The Bold and the Beautiful)   9140154657058392803   \n","3                                    Acronym  -2103986527722712835   \n","4       Michigan Wolverines men's basketball  -4628267852301989966   \n","\n","                                           questions  \\\n","0  [{'input_text': 'who is the owner of the manda...   \n","1  [{'input_text': 'what is the shannara chronicl...   \n","2  [{'input_text': 'who played taylor on the bold...   \n","3  [{'input_text': 'what do you call initials tha...   \n","4  [{'input_text': 'when did michigan last win a ...   \n","\n","                                             answers  has_correct_context  \\\n","0  [{'candidate_id': 0, 'input_text': 'short', 's...                 True   \n","1  [{'candidate_id': 0, 'input_text': 'short', 's...                 True   \n","2  [{'candidate_id': 0, 'input_text': 'short', 's...                 True   \n","3  [{'candidate_id': 0, 'input_text': 'short', 's...                 True   \n","4  [{'candidate_id': 0, 'input_text': 'short', 's...                 True   \n","\n","                                            contexts  \n","0  Mandalay Bay Location Paradise , Nevada , U.S....  \n","1  The Shannara Chronicles Genre Fantasy Created ...  \n","2  Taylor Hayes Hunter Tylo as Taylor Hayes The B...  \n","3  An acronym is a word or name formed as an abbr...  \n","4  Michigan Wolverines men 's basketball 2018 -- ...  \n"]}],"source":["# Inspect the dataset\n","print(dataset)\n","\n","print(dataset[\"train\"][1])\n","\n","import pandas as pd\n","\n","train_df = pd.DataFrame(dataset[\"train\"])\n","dev_df = pd.DataFrame(dataset[\"dev\"])\n","\n","print(\"Training dataset:\")\n","print(train_df.head())\n","\n","print(\"\\nDevelopment dataset:\")\n","print(dev_df.head())\n"]},{"cell_type":"markdown","metadata":{"id":"5rm4aL2jZh80"},"source":["# load model\n"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"v3DYInGnhqVm","executionInfo":{"status":"ok","timestamp":1734293299263,"user_tz":300,"elapsed":188,"user":{"displayName":"mia yu","userId":"18290570536680028134"}}},"outputs":[],"source":["import torch\n","from transformers import DistilBertModel\n","\n","class DistilBertForQAWithType(torch.nn.Module):\n","    def __init__(self, model_name=\"distilbert-base-uncased\"):\n","        super(DistilBertForQAWithType, self).__init__()\n","        self.model = DistilBertModel.from_pretrained(model_name)\n","        hidden_size = self.model.config.hidden_size\n","\n","        # Outputs for start and end spans (512 logits each)\n","        # self.qa_outputs = torch.nn.Linear(hidden_size, 2)\n","        self.start_layer = torch.nn.Linear(hidden_size, 1)\n","        self.end_layer = torch.nn.Linear(hidden_size, 1)\n","\n","        # Outputs for question type (2 logits)\n","        self.answer_layer = torch.nn.Linear(hidden_size, 2)\n","\n","        self.dropout = torch.nn.Dropout(0.1)\n","\n","    def forward(self, input_ids, attention_mask):\n","        # Get hidden states from DistilBERT\n","        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n","        sequence_output = outputs.last_hidden_state  # Shape: (batch_size, seq_len, hidden_size)\n","\n","        # Question type logits (using CLS token representation)\n","        cls_output = sequence_output[:, 0, :]  # Shape: (batch_size, hidden_size)\n","\n","\n","        # Start and end logits\n","\n","        # qa_logits = self.qa_outputs(sequence_output)  # Shape: (batch_size, seq_len, 2)\n","        # start_logits, end_logits = qa_logits.split(1, dim=-1)  # Each: (batch_size, seq_len, 1)\n","        # start_logits = start_logits.squeeze(-1)  # Shape: (batch_size, seq_len)\n","        # end_logits = end_logits.squeeze(-1)  # Shape: (batch_size, seq_len)\n","\n","        start_logits = self.start_layer(sequence_output).squeeze(-1)  # Shape: [batch_size, seq_len]\n","        end_logits = self.end_layer(sequence_output).squeeze(-1)      # Shape: [batch_size, seq_len]\n","\n","        answer_logits = self.answer_layer(self.dropout(cls_output))  # Shape: (batch_size, 2)\n","\n","        return start_logits, end_logits, answer_logits\n"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"gTxJoZVeZo5E","executionInfo":{"status":"ok","timestamp":1734293362615,"user_tz":300,"elapsed":224,"user":{"displayName":"mia yu","userId":"18290570536680028134"}}},"outputs":[],"source":["from transformers import DistilBertTokenizerFast\n","\n","def load_model(model_name=\"distilbert-base-uncased\"):\n","    \"\"\"\n","    Load the tokenizer and initialize the QA model with question type prediction.\n","\n","    Args:\n","        model_name (str): Pretrained model name or path.\n","\n","    Returns:\n","        model (DistilBertForQAWithType): Initialized model.\n","        tokenizer (DistilBertTokenizerFast): Pretrained tokenizer.\n","    \"\"\"\n","    # Load tokenizer\n","    tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n","\n","    # Load custom model\n","    model = DistilBertForQAWithType(model_name=model_name)\n","\n","    return model, tokenizer\n"]},{"cell_type":"markdown","metadata":{"id":"eLuiEBL3bjN1"},"source":["# preprocess and tokenize"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"UOwm1B84D4Z0","executionInfo":{"status":"ok","timestamp":1734292548618,"user_tz":300,"elapsed":6,"user":{"displayName":"mia yu","userId":"18290570536680028134"}}},"outputs":[],"source":["from torch.utils.data import Dataset\n","import torch\n","# from transformers import AutoTokenizer\n","\n","def find_encoded(example, tokenizer, max_seq_length=512):\n","  \"\"\"\n","    Encodes the input data (question, context, and answer) using a tokenizer, and maps the character-level\n","    indices of the answer to token-level indices within the context.\n","\n","    Parameters:\n","    - example: A dictionary containing the following keys:\n","        - \"questions\": List of question dictionaries with \"input_text\" (the question text).\n","        - \"contexts\": The context text (string) where the answer is located.\n","        - \"answers\": List of answer dictionaries containing:\n","            - \"span_start\": The character index where the answer starts in the context.\n","            - \"span_end\": The character index where the answer ends in the context.\n","            - \"input_text\": The type of question (e.g., \"short\" or \"long\").\n","    - tokenizer: A tokenizer instance (e.g., from Hugging Face's Transformers library) used for encoding.\n","\n","    Returns:\n","    A dictionary containing:\n","    - \"input_ids\": The tokenized input IDs.\n","    - \"attention_mask\": Attention mask for the input sequence.\n","    - \"start_positions\": The token index where the answer starts in the context.\n","    - \"end_positions\": The token index where the answer ends in the context.\n","    - \"question_type\": 1 if the answer type is \"short\", otherwise 0.\n","    \"\"\"\n","\n","  question = example[\"questions\"][0][\"input_text\"]\n","  context = example[\"contexts\"]\n","  answer = example[\"answers\"][0]\n","  # answer = example['answers'][0] if example['answers'] else None\n","\n","  encoded = tokenizer.encode_plus(\n","      question,\n","      context,\n","      max_length = 512,\n","      truncation=True,\n","      add_special_tokens=True,\n","      padding='max_length',\n","      return_attention_mask=True,\n","      return_offsets_mapping = True,\n","      return_token_type_ids = True\n","  )\n","\n","  ## find char idx\n","  start_char_idx = answer['span_start']\n","  end_char_idx = answer['span_end']\n","  answer_label = 1 if answer[\"input_text\"] == \"short\" else 0\n","  offset_mapping= encoded['offset_mapping']\n","  token_type_ids = encoded['token_type_ids']\n","\n","  # input_ids = inputs[\"input_ids\"].squeeze(0)\n","  # attention_mask = inputs[\"attention_mask\"].squeeze(0)\n","\n","\n","  ## convert from char idx to token idx\n","  start_token_idx, end_token_idx = 0,0\n","  context_start_token = token_type_ids.index(1)\n","  context_end_token = len(token_type_ids) - token_type_ids[::-1].index(1) - 1\n","\n","\n","  if answer_label == 1:\n","    for i, offset in enumerate(offset_mapping[context_start_token: context_end_token+1]):\n","      i = i + context_start_token\n","      ## update to be token idx within a context\n","      if offset[0] == offset[1] and offset[1] ==0:\n","        break\n","      if offset[0] <= start_char_idx and offset[1] > start_char_idx:\n","        start_token_idx = i\n","\n","      if offset[0] <= end_char_idx and offset[1] >= end_char_idx:\n","        end_token_idx = i\n","      if start_token_idx != 0 and end_token_idx!=0:\n","        break\n","\n","  return {\n","        \"input_ids\": encoded[\"input_ids\"],\n","        \"attention_mask\": encoded[\"attention_mask\"],\n","        \"start_positions\": start_token_idx,\n","        \"end_positions\": end_token_idx,\n","        \"answer_label\": answer_label\n","    }\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"_0fvMrHFFMt5","colab":{"base_uri":"https://localhost:8080/","height":433,"referenced_widgets":["76cca57478c14d8a90b6c40723ae359b","42d829723d594b12bdf830b4eb376c7e","d40cf66093944fbc942e4ad8b71cc277","81f08e03061343a09de81fe6123b2aa1","952cc348fce6475f9a88353dcfbb5330","dd912f5ae6ca4c88b400e5f2a7926c8b","67aab0371950497a96e762681f38b92e","e0927fb67189473e9f40cc1db7eb14db","6669197d664647c3ad46f8607c997fe3","2ab228aefcdc46ada3b6243fa1ec32c9","e6948cbac63c43888659f7f8848249b9","ae05fd298e3c4d958e5223e93d44c831","c2ae7c5a4520430bbbfa25afdb41426f","a3b9967f3cd04b2ba9ab04580b056971","fd1a249fb5d84ddcbe44e0f6958e900c","f780ca0ec2a0441d9b34abf62c253cf3","360c083a6d1448319bf1aeea72008a84","61e55af82ddf46389c8f705429642f49","f0d80fce2f1e47e7af60aa4921243895","7286a20db44f4ff68d5f22a5a0d058e9","619472f3ce5a474e8765c05a40978f2d","460b4d25084f45aeb347f79272043f9a","64c9a20dd1164e579b9e74cb684cf3b7","54a33275ad9b4625b2ed4602cef8aa8d","cf8799146b1548a9b182ab0e3c042eb5","bcd9300ed26c4558a5f97f65c22f25c8","02e03efd0e2346698fa677e1a74eadb3","48636506315b4de4a84acd903375d87f","e2e1a7c110e64e7595a61f96d78a1696","a33a76534bf64379a9899c07fa6ff251","4ac31993c4404cce9745a91e6395975e","c75b1ae635ed4075a8055d96d7a43877","d9731883bed14743aed5334ef431ca43","8f61ab886bc6438c9fffa50be7e6e112","a2478702b2864bea9dc18d8289bd7545","22e4787b59be4293a1c28e5850ed8ced","97a0942bc08b445b8a17b5d11895db19","d34f2709cadc41bba263584162bd5da2","d78deab68ccf4330b63d611b2ca8cf4f","5923f1adda754bdf8f4c3764d86536e1","fd34676771a448bba247717556e01d87","682413848696467ab07cdde6c633a9e0","a9ce2dbe40fe4f34a561dfcbd9480094","af6eba84155b409aa96afbc858b38c98"]},"executionInfo":{"status":"ok","timestamp":1734292551246,"user_tz":300,"elapsed":2633,"user":{"displayName":"mia yu","userId":"18290570536680028134"}},"outputId":"c37fd0b1-7457-463c-f770-8214ffa25c23"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'name': \"Stephanie Edwards (Grey's Anatomy)\", 'id': '5495190773098085777', 'questions': [{'input_text': \"when does stephanie die in grey's anatomy\"}], 'answers': [{'candidate_id': 0, 'input_text': 'short', 'span_end': 324, 'span_start': 296, 'span_text': \"`` Ring of Fire '' ( 13.24 )\"}], 'has_correct_context': True, 'contexts': \"Dr. Stephanie Edwards Grey 's Anatomy character The Season 12 Promotional Photo of Jerrika Hinton as Stephanie Edwards First appearance Going , Going , Gone ( 9.01 ) September 27 , 2012 ( as recurring cast ) `` Seal Our Fate '' ( 10.01 ) September 26 , 2013 ( as series regular ) Last appearance `` Ring of Fire '' ( 13.24 ) May 18 , 2017 Created by Shonda Rhimes Portrayed by Jerrika Hinton Information Full name Stephanie Edwards Nickname ( s ) Grumpy Steph Dr. Lavender Title M.D. Significant other ( s ) Jackson Avery Kyle Diaz ( deceased )\"}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76cca57478c14d8a90b6c40723ae359b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae05fd298e3c4d958e5223e93d44c831"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64c9a20dd1164e579b9e74cb684cf3b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f61ab886bc6438c9fffa50be7e6e112"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n","The class this function is called from is 'DistilBertTokenizerFast'.\n"]},{"output_type":"stream","name":"stdout","text":["Input IDs: [101, 2043, 2515, 11496, 3280, 1999, 4462, 1005, 1055, 13336, 102, 2852, 1012, 11496, 7380, 4462, 1005, 1055, 13336, 2839, 1996, 2161, 2260, 10319, 6302, 1997, 15333, 18752, 2912, 9374, 2239, 2004, 11496, 7380, 2034, 3311, 2183, 1010, 2183, 1010, 2908, 1006, 1023, 1012, 5890, 1007, 2244, 2676, 1010, 2262, 1006, 2004, 10694, 3459, 1007, 1036, 1036, 7744, 2256, 6580, 1005, 1005, 1006, 2184, 1012, 5890, 1007, 2244, 2656, 1010, 2286, 1006, 2004, 2186, 3180, 1007, 2197, 3311, 1036, 1036, 3614, 1997, 2543, 1005, 1005, 1006, 2410, 1012, 2484, 1007, 2089, 2324, 1010, 2418, 2580, 2011, 26822, 8943, 1054, 14341, 2229, 6791, 2011, 15333, 18752, 2912, 9374, 2239, 2592, 2440, 2171, 11496, 7380, 8367, 1006, 1055, 1007, 24665, 24237, 2100, 3357, 2232, 2852, 1012, 20920, 2516, 1049, 1012, 1040, 1012, 3278, 2060, 1006, 1055, 1007, 4027, 12140, 7648, 12526, 1006, 10181, 1007, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","Start Positions: 78\n","End Positions: 89\n","answer label: 1\n"]}],"source":["## unit test find_encoded\n","example = dataset[\"train\"][0]\n","print(example)\n","\n","# Load a tokenizer (you can replace 'bert-base-uncased' with your model's tokenizer)\n","tokenizer = DistilBertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n","\n","# Call the function and print results\n","result = find_encoded(example, tokenizer)\n","\n","# Print the result\n","print(\"Input IDs:\", result[\"input_ids\"])\n","print(\"Attention Mask:\", result[\"attention_mask\"])\n","print(\"Start Positions:\", result[\"start_positions\"])\n","print(\"End Positions:\", result[\"end_positions\"])\n","print(\"answer label:\", result[\"answer_label\"])"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"muahEgbZblSY","executionInfo":{"status":"ok","timestamp":1734292551246,"user_tz":300,"elapsed":3,"user":{"displayName":"mia yu","userId":"18290570536680028134"}}},"outputs":[],"source":["from torch.utils.data import Dataset\n","import torch\n","from transformers import DistilBertModel\n","\n","class CustomDatasetForQAWithType(Dataset):\n","  \"\"\"\n","      A customized Pytorch dataset for question answering.\n","  \"\"\"\n","  def __init__(self, data, tokenizer, max_length=512):\n","      self.data = data\n","      self.tokenizer = tokenizer\n","      self.max_length = max_length\n","\n","  def __len__(self):\n","      return len(self.data)\n","\n","  def __getitem__(self, idx):\n","      example = self.data[idx]\n","      encoded = find_encoded(example, self.tokenizer)\n","\n","      return {\n","          'input_ids': torch.tensor(encoded['input_ids']).squeeze(),\n","          'attention_mask': torch.tensor(encoded['attention_mask']).squeeze(),\n","          'start_positions': torch.tensor(encoded['start_positions']),\n","          'end_positions': torch.tensor(encoded['end_positions']),\n","          'answer_label': torch.tensor(encoded['answer_label']),\n","      }"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"SX1zP7ZsbvKb","executionInfo":{"status":"ok","timestamp":1734292551549,"user_tz":300,"elapsed":305,"user":{"displayName":"mia yu","userId":"18290570536680028134"}}},"outputs":[],"source":["from torch.utils.data import DataLoader, Dataset\n","\n","def preprocess_and_tokenize(data, tokenizer, batch_size):\n","  \"\"\"\n","      Preprocess and tokenize raw data and return a data loader.\n","\n","      Parameters\n","      - data: A list or collection of input examples\n","      - tokenizer: A tokenizer instance\n","      - batch_size: batch size\n","\n","      Returns:\n","      - a dataloader instance\n","  \"\"\"\n","\n","  dataset = CustomDatasetForQAWithType(data, tokenizer)\n","  return DataLoader(dataset, batch_size=batch_size, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"5FCjGPMqcEzF"},"source":["# train loop"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"xc_jtM7kh3uq","executionInfo":{"status":"ok","timestamp":1734292551549,"user_tz":300,"elapsed":8,"user":{"displayName":"mia yu","userId":"18290570536680028134"}}},"outputs":[],"source":["# from tqdm import tqdm\n","# from torch.nn.functional import softmax, log_softmax, nll_loss\n","\n","# def train_loop(model, train_data_loader, validation_data_loader, epochs, optimizer, lr_scheduler, device):\n","#   \"\"\"\n","#     Trains and validates a model for a specified number of epochs, using given data loaders, optimizer,\n","#     and learning rate scheduler. Tracks training and validation losses across epochs.\n","\n","#     Parameters:\n","#     - model: The model to be trained and validated.\n","#     - train_data_loader: DataLoader for the training dataset.\n","#     - validation_data_loader: DataLoader for the validation dataset.\n","#     - epochs: Number of training epochs.\n","#     - optimizer: Optimizer for updating model parameters (e.g., AdamW).\n","#     - lr_scheduler: Learning rate scheduler for dynamically adjusting the learning rate.\n","\n","#     Returns:\n","#     - train_losses: List of average training losses for each epoch.\n","#     - val_losses: List of average validation losses for each epoch.\n","#   \"\"\"\n","\n","#   # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","#   model.to(device)\n","\n","#   # optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","#   # criterion_span = torch.nn.CrossEntropyLoss()\n","#   # criterion_type = torch.nn.CrossEntropyLoss()\n","\n","#   train_losses = []\n","#   val_losses = []\n","\n","#   for epoch in range(epochs):\n","#       model.train()\n","#       train_loss = 0\n","\n","#       for batch in tqdm(train_data_loader, desc=f\"Training Epoch {epoch+1}/{epochs}\"):\n","#           input_ids = batch[\"input_ids\"].to(device)\n","#           attention_mask = batch[\"attention_mask\"].to(device)\n","#           start_positions = batch[\"start_positions\"].to(device)\n","#           end_positions = batch[\"end_positions\"].to(device)\n","#           answer_label = batch[\"answer_label\"].to(device)\n","\n","#           optimizer.zero_grad()\n","#           start_logits, end_logits, answer_logits = model(input_ids, attention_mask)\n","\n","#           # Compute losses\n","#           # loss_start = criterion_span(start_logits, start_positions)\n","#           # loss_end = criterion_span(end_logits, end_positions)\n","#           # loss_type = criterion_type(answer_logits, answer_label)\n","\n","#           # start_log_probs = log_softmax(start_logits, dim=-1)  # Log probabilities\n","#           # end_log_probs = log_softmax(end_logits, dim=-1)\n","#           # answer_log_probs = log_softmax(answer_logits, dim=-1)\n","\n","#           # loss_start = nll_loss(start_log_probs, start_positions)  # Negative log likelihood\n","#           # loss_end = nll_loss(end_log_probs, end_positions)\n","#           # loss_type = nll_loss(answer_log_probs, answer_label)\n","\n","#           batch_size = input_ids.size(0)\n","\n","#           start_positions = start_positions[:batch_size]\n","#           end_positions = end_positions[:batch_size]\n","#           answer_label = answer_label[:batch_size]\n","\n","#           loss_start = nll_loss(log_softmax(start_logits, dim=-1), start_positions)\n","#           loss_end = nll_loss(log_softmax(end_logits, dim=-1), end_positions)\n","#           loss_type = nll_loss(log_softmax(answer_logits, dim=-1), answer_label)\n","\n","\n","#           loss = loss_start + loss_end + loss_type\n","#           train_loss += loss.item()\n","\n","#           loss.backward()\n","#           optimizer.step()\n","#           lr_scheduler.step()\n","#           optimizer.step()\n","\n","#       train_losses.append(train_loss / len(train_data_loader))\n","\n","#       average_train_loss = train_loss / len(train_data_loader)\n","#       train_losses.append(average_train_loss)\n","#       print(f\"Epoch {epoch+1}: Train Loss = {average_train_loss:.4f}\")\n","\n","\n","#       # Validation step\n","#       val_loss = 0\n","#       model.eval()\n","#       with torch.no_grad():\n","#           for batch in tqdm(validation_data_loader, desc=\"Validating\"):\n","#               input_ids = batch[\"input_ids\"].to(device)\n","#               attention_mask = batch[\"attention_mask\"].to(device)\n","#               start_positions = batch[\"start_positions\"].to(device)\n","#               end_positions = batch[\"end_positions\"].to(device)\n","#               answer_label = batch[\"answer_label\"].to(device)\n","\n","#               start_logits, end_logits, type_logits = model(input_ids, attention_mask)\n","\n","#               # Compute losses\n","#               # loss_start = criterion_span(start_logits, start_positions)\n","#               # loss_end = criterion_span(end_logits, end_positions)\n","#               # loss_type = criterion_type(type_logits, answer_label)\n","#               # start_log_probs = log_softmax(start_logits, dim=-1)  # Log probabilities\n","#               # end_log_probs = log_softmax(end_logits, dim=-1)\n","#               # answer_log_probs = log_softmax(answer_logits, dim=-1)\n","\n","#               # loss_start = nll_loss(start_log_probs, start_positions)  # Negative log likelihood\n","#               # loss_end = nll_loss(end_log_probs, end_positions)\n","#               # loss_type = nll_loss(answer_log_probs, answer_label)\n","#               batch_size = input_ids.size(0)  # dynamic batch size adjustment\n","\n","#               start_positions = start_positions[:batch_size]\n","#               end_positions = end_positions[:batch_size]\n","#               answer_label = answer_label[:batch_size]\n","\n","#               loss_start = nll_loss(log_softmax(start_logits, dim=-1), start_positions)\n","#               loss_end = nll_loss(log_softmax(end_logits, dim=-1), end_positions)\n","#               loss_type = nll_loss(log_softmax(answer_logits, dim=-1), answer_label)\n","\n","\n","#               loss = loss_start + loss_end + loss_type\n","#               val_loss += loss.item()\n","\n","#       # val_losses.append(val_loss / len(validation_data_loader))\n","#       # print(f\"Epoch {epoch+1}: Train Loss = {train_losses[-1]:.4f}, Val Loss = {val_losses[-1]:.4f}\")\n","#       average_val_loss = val_loss / len(validation_data_loader)\n","#       val_losses.append(average_val_loss)\n","#       print(f\"Epoch {epoch+1}: Val Loss = {average_val_loss:.4f}\")\n","\n","#   return train_losses, val_losses\n"]},{"cell_type":"code","source":["from tqdm import tqdm\n","from torch.nn.functional import softmax, log_softmax, nll_loss\n","\n","from torch.nn import CrossEntropyLoss\n","def compute_loss(start_logits, end_logits, answer_logits, start_positions, end_positions, answer_labels):\n","    \"\"\"\n","      Compute cross entropy losses for start position, end position, and answer label and return their sum.\n","\n","      Parameters\n","      - start_logits: logits for start position, Shape: [batch_size, seq_len]\n","      - end_logits: logits for end position, Shape: [batch_size, seq_len]\n","      - answer_logits: logits for answer label, Shape: [batch_size, 2]\n","      - start_positions: start position, Shape: [batch_size]\n","      - end_positions: end position, Shape: [batch_size]\n","\n","      Returns:\n","      - a scalar loss\n","    \"\"\"\n","\n","    start_log_probs = log_softmax(start_logits, dim=-1)  # Log probabilities\n","    end_log_probs = log_softmax(end_logits, dim=-1)\n","    answer_log_probs = log_softmax(answer_logits, dim=-1)\n","\n","    start_loss = nll_loss(start_log_probs, start_positions)  # Negative log likelihood\n","    end_loss = nll_loss(end_log_probs, end_positions)\n","    answer_loss = nll_loss(answer_log_probs, answer_labels)\n","\n","\n","    return start_loss + end_loss + answer_loss\n","def get_val_loss(model,\n","             validation_data_loader,\n","             device):\n","    \"\"\"\n","      Evaluate average validation loss over the validation dataset.\n","\n","      Parameters\n","      - model: BertShortAnswerModel\n","      - validation_data_loader: validation data loader\n","      - device: cpu or cuda\n","\n","      Returns:\n","      - average validation loss\n","    \"\"\"\n","    model.eval()\n","    total_loss = 0\n","\n","    # num_epochs = 2\n","    with torch.no_grad():\n","        for batch in tqdm(validation_data_loader, desc=\"Validating\"):\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            start_positions = batch['start_positions'].to(device) # Shape: [batch_size]\n","            end_positions = batch['end_positions'].to(device)     # Shape: [batch_size]\n","            answer_label = batch['answer_label'].to(device) # Shape: [batch_size]\n","\n","            start_logits, end_logits, answer_logits = model(input_ids, attention_mask) # Shape: [batch_size, seq_len] or [batch_size, 2]\n","            loss = compute_loss(start_logits, end_logits, answer_logits, start_positions, end_positions, answer_label)\n","            total_loss += loss.item()\n","\n","\n","    avg_loss = total_loss / len(validation_data_loader)\n","    return avg_loss\n","\n","\n","def train_loop(model,\n","          num_epochs,\n","          optimizer,\n","          train_data_loader,\n","          validation_data_loader,\n","          lr_scheduler,\n","          device):\n","    \"\"\"\n","      Train the model for a specified number of epochs.\n","\n","      Parameters\n","      - model: BertShortAnswerModel\n","      - num_epochs: number of epochs\n","      - optimizer: optimizer\n","      - train_data_loader: training data loader\n","      - validation_data_loader: validation data loader\n","      - lr_scheduler: learning rate scheduler\n","      - device: cpu or cuda\n","\n","      Returns:\n","      - a tuple of (train_losses, val_losses)\n","\n","    \"\"\"\n","    # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","    train_losses, val_losses = [], []\n","    # epochs = 2\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_train_loss = 0\n","\n","        print(f\"Epoch {epoch + 1} training:\")\n","\n","        for batch in tqdm(train_data_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            start_positions = batch['start_positions'].to(device)\n","            end_positions = batch['end_positions'].to(device)\n","            answer_label = batch['answer_label'].to(device)\n","\n","            start_logits, end_logits, answer_logits = model(input_ids, attention_mask)\n","            loss = compute_loss(start_logits, end_logits, answer_logits, start_positions, end_positions, answer_label)\n","            total_train_loss += loss.item()\n","\n","            loss.backward()\n","            optimizer.step() # Update model parameters\n","            lr_scheduler.step() # Learning rate scheduler\n","            optimizer.zero_grad() # Reset gradients\n","\n","\n","        avg_train_loss = total_train_loss / len(train_data_loader)\n","        print(f\"Training Loss: {avg_train_loss}\")\n","\n","        model.eval()\n","\n","        print(f\"Epoch {epoch + 1} validation:\")\n","\n","        val_loss = get_val_loss(model, validation_data_loader, device)\n","        print(f\"Validation Loss: {val_loss}\")\n","        print(\"------------------------------------\")\n","\n","        train_losses.append(avg_train_loss)\n","        val_losses.append(val_loss)\n","\n","    return train_losses, val_losses"],"metadata":{"id":"adVqmgPkmPmu","executionInfo":{"status":"ok","timestamp":1734293643037,"user_tz":300,"elapsed":208,"user":{"displayName":"mia yu","userId":"18290570536680028134"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LwyOcwfhci5s"},"source":["# evl loop **"]},{"cell_type":"markdown","source":["## Average"],"metadata":{"id":"2sgehiTz4wNP"}},{"cell_type":"code","execution_count":14,"metadata":{"id":"0_64lW3OeG0W","executionInfo":{"status":"ok","timestamp":1734292551549,"user_tz":300,"elapsed":7,"user":{"displayName":"mia yu","userId":"18290570536680028134"}}},"outputs":[],"source":["from transformers import DistilBertTokenizerFast\n","from collections import Counter\n","def compute_metrics(input_ids, start_logits, end_logits, start_positions, end_positions):\n","    \"\"\"\n","      Count the number of true positives, false positives, and false negatives for a batch of predictions.\n","\n","      Parameters\n","      - input_ids: encodings for input text, Shape: [batch_size, seq_len]\n","      - start_logits: logits for start position, Shape: [batch_size, seq_len]\n","      - end_logits: logits for end position, Shape: [batch_size, seq_len]\n","      - start_positions: start position, Shape: [batch_size]\n","      - end_positions: end position, Shape: [batch_size]\n","\n","      Returns:\n","      - a tuple of (total_tp, total_fp, total_fn)\n","    \"\"\"\n","    batch_size = start_logits.shape[0]\n","\n","    total_precision = 0.0\n","    total_recall = 0.0\n","    total_f1 = 0.0\n","\n","    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n","\n","\n","    for i in range(batch_size):\n","        pred_start = torch.argmax(start_logits[i])\n","        pred_end = torch.argmax(end_logits[i])\n","        act_start = start_positions[i]\n","        act_end = end_positions[i]\n","\n","\n","        if pred_start > pred_end:\n","            continue\n","\n","        pred_tokens = tokenizer.convert_ids_to_tokens(input_ids[i][pred_start:pred_end+1])\n","        act_tokens = tokenizer.convert_ids_to_tokens(input_ids[i][act_start:act_end+1])\n","\n","        pred_counter = Counter(pred_tokens)\n","        act_counter = Counter(act_tokens)\n","\n","        tp = sum((pred_counter & act_counter).values())\n","        fp = sum((pred_counter - act_counter).values())\n","        fn = sum((act_counter - pred_counter).values())\n","\n","        precision = tp / (tp + fp) if tp + fp > 0 else 0\n","        recall = tp / (tp + fn) if tp + fn > 0 else 0\n","        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n","\n","        total_precision += precision\n","        total_recall += recall\n","        total_f1 += f1_score\n","\n","    avg_precision =  total_precision / batch_size\n","    avg_recall = total_recall / batch_size\n","    avg_f1 = total_f1 / batch_size\n","\n","    return avg_precision, avg_recall, avg_f1\n","\n","def eval_loop(model, validation_data_loader):\n","    \"\"\"\n","      Evaluate precision, recall, and f1 score over the validation dataset.\n","\n","      Parameters\n","      - model: BertShortAnswerModel\n","      - validation_data_loader: validation data loader\n","      - device: cpu or cuda\n","\n","      Returns:\n","      - a tuple of (precision, recall, f1)\n","    \"\"\"\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    model.eval()\n","\n","    # total_tp = 0\n","    # total_fp = 0\n","    # total_fn = 0\n","\n","    total_precision = 0.0\n","    total_recall = 0.0\n","    total_f1 = 0.0\n","\n","    print(\"Evaluating metrics:\")\n","    # progress_bar = tqdm(range(len(validation_data_loader)))\n","\n","    with torch.no_grad():\n","        for batch in tqdm(range(len(validation_data_loader))):\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            start_positions = batch['start_positions'].to(device)\n","            end_positions = batch['end_positions'].to(device)\n","            # question_type = batch[\"question_type\"].to(device)\n","\n","            start_logits, end_logits, _ = model(input_ids, attention_mask)\n","            precision, recall, f1 = compute_metrics(input_ids, start_logits, end_logits, start_positions, end_positions)\n","\n","            total_precision += precision\n","            total_recall += recall\n","            total_f1 += f1\n","\n","            # progress_bar.update(1)\n","\n","    precision = total_precision / len(validation_data_loader)\n","    recall = total_recall / len(validation_data_loader)\n","    f1_score = total_f1 / len(validation_data_loader)\n","\n","    return precision, recall, f1_score"]},{"cell_type":"markdown","source":["## total"],"metadata":{"id":"6T2TN-di4zWS"}},{"cell_type":"code","source":["from transformers import DistilBertTokenizerFast\n","def compute_metrics_2(input_ids, start_logits, end_logits, start_positions, end_positions):\n","  # 2 a helper function for eval_loop 2 upgrade\n","    num_true_positive = 0\n","    num_false_positive = 0\n","    num_false_negative = 0\n","\n","    batch_size = start_logits.shape[0]\n","\n","    for i in range(batch_size):\n","        pred_start = torch.argmax(start_logits[i]).item()\n","        pred_end = torch.argmax(end_logits[i]).item()\n","        act_start = start_positions[i].item()\n","        act_end = end_positions[i].item()\n","\n","        # if invalid predictions\n","        if pred_start > pred_end:\n","            continue\n","\n","        tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n","\n","        pred_tokens = tokenizer.convert_ids_to_tokens(input_ids[i][pred_start:pred_end+1])\n","        act_tokens = tokenizer.convert_ids_to_tokens(input_ids[i][act_start:act_end+1])\n","\n","        pred_counter = Counter(pred_tokens)\n","        act_counter = Counter(act_tokens)\n","\n","        tp = sum((pred_counter & act_counter).values())\n","        fp = sum((pred_counter - act_counter).values())\n","        fn = sum((act_counter - pred_counter).values())\n","\n","        num_true_positive+= tp\n","        num_false_positive+= fp\n","        num_false_negative+= fn\n","\n","    return num_true_positive, num_false_positive, num_false_negative\n","\n","\n","def eval_loop_2(model, data_loader):\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    model.eval()\n","\n","    total_true_positive = 0\n","    total_false_positive = 0\n","    total_false_negative = 0\n","\n","    with torch.no_grad():\n","        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            start_positions = batch[\"start_positions\"].to(device)\n","            end_positions = batch[\"end_positions\"].to(device)\n","\n","            start_logits, end_logits, _ = model(input_ids, attention_mask)\n","\n","            # Compute metrics for the batch\n","            true_positive, false_positive, false_negative = compute_metrics_2(input_ids,\n","                start_logits, end_logits, start_positions, end_positions\n","            )\n","\n","            total_true_positive += true_positive\n","            total_false_positive += false_positive\n","            total_false_negative += false_negative\n","\n","    # Average metrics across batches\n","    precision = total_true_positive / (total_true_positive + total_false_positive) if (total_true_positive + total_false_positive) > 0 else 0\n","    recall = total_true_positive / (total_true_positive + total_false_negative) if (total_true_positive + total_false_negative) > 0 else 0\n","    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n","\n","\n","    return precision, recall, f1_score"],"metadata":{"id":"evQOMZbI3F9a","executionInfo":{"status":"ok","timestamp":1734292551549,"user_tz":300,"elapsed":7,"user":{"displayName":"mia yu","userId":"18290570536680028134"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kA-tiqkDcveX"},"source":["# main"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"KS9ZcP-umJLN","colab":{"base_uri":"https://localhost:8080/","height":436},"outputId":"28a21cc1-b823-437d-9262-a90f3915d914","executionInfo":{"status":"error","timestamp":1734294154539,"user_tz":300,"elapsed":1363,"user":{"displayName":"mia yu","userId":"18290570536680028134"}}},"outputs":[{"output_type":"error","ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 45.06 MiB is free. Process 5881 has 14.70 GiB memory in use. Of the allocated memory 14.49 GiB is allocated by PyTorch, and 86.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-44-31b672d28bf5>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     )\n\u001b[0;32m-> 1326\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 45.06 MiB is free. Process 5881 has 14.70 GiB memory in use. Of the allocated memory 14.49 GiB is allocated by PyTorch, and 86.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"]}],"source":["from transformers import get_scheduler\n","\n","# def main():\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","batch_size = 64\n","num_epochs = 2\n","\n","model, tokenizer = load_model()\n","model.to(device)\n","\n","train, validation = load_data(dataset)\n","# print(train[:2])\n","\n","train_data_loader = preprocess_and_tokenize(train, tokenizer, batch_size=batch_size)\n","validation_data_loader = preprocess_and_tokenize(validation, tokenizer, batch_size=batch_size)\n","\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n","lr_scheduler = get_scheduler(\n","  \"linear\",\n","  optimizer = optimizer,\n","  num_warmup_steps = 50,\n","  num_training_steps = len(train_data_loader) * num_epochs\n",")\n","\n","train_losses, val_losses = train_loop(model,\n","          num_epochs,\n","          optimizer,\n","          train_data_loader,\n","          validation_data_loader,\n","          lr_scheduler,\n","          device)\n","\n","\n","# train_losses, val_losses = train_loop(model, num_epochs, optimizer,train_data_loader,validation_data_loader,lr_scheduler)\n","precision, recall, f1_score  = eval_loop(model, validation_data_loader)\n","\n","print(\"\")\n","print(\"eval_loop\")\n","print(\"PRECISION: \", precision)\n","print(\"RECALL: \", recall)\n","print(\"F1-SCORE: \", f1_score)\n","\n","precision, recall, f1_score  = eval_loop_2(model, validation_data_loader)\n","\n","print(\"\")\n","print(\"eval_loop_total\")\n","print(\"PRECISION: \", precision)\n","print(\"RECALL: \", recall)\n","print(\"F1-SCORE: \", f1_score)\n","\n","\n","# if __name__ == \"__main__\":\n","#   main()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1ltpbcm9393_ZK96Glbr1RrMxFaq0vOc0","timestamp":1733460630820}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"77ed9a8bb0cb46f0bb46df98c91b2dca":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d541d825f5d549d1b7d992b099348f42","IPY_MODEL_a121333de8304e6b9acdaefcea87c46e","IPY_MODEL_5d4b80a451a24a539afcd29e2e92a360"],"layout":"IPY_MODEL_4f512deb6f224f86ab260ba2a71aface"}},"d541d825f5d549d1b7d992b099348f42":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e2961d7476b43a8bbb901c3ceb3faa1","placeholder":"​","style":"IPY_MODEL_db074b91eb0b4256b05011c09d241d31","value":"Generating train split: "}},"a121333de8304e6b9acdaefcea87c46e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_94c3a200d9394462990e9dbed3e78b4f","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9a52eaa608004d4b81d907d25c35bb93","value":1}},"5d4b80a451a24a539afcd29e2e92a360":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_65e80179c64a441cb0fd8ae48c7add95","placeholder":"​","style":"IPY_MODEL_428050ca55d049d4b1a7ef2c52a49aaf","value":" 27866/0 [00:02&lt;00:00, 13920.96 examples/s]"}},"4f512deb6f224f86ab260ba2a71aface":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e2961d7476b43a8bbb901c3ceb3faa1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db074b91eb0b4256b05011c09d241d31":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"94c3a200d9394462990e9dbed3e78b4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"9a52eaa608004d4b81d907d25c35bb93":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"65e80179c64a441cb0fd8ae48c7add95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"428050ca55d049d4b1a7ef2c52a49aaf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41ea3082066c47959cf3c83404bb4326":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f489a1d89ee24e3d9e4cdb8ab11d18ac","IPY_MODEL_d1cf3cb3f4b847889222dcc3104eaa3c","IPY_MODEL_8d6a43d53f384e32bcc99aea31b92e79"],"layout":"IPY_MODEL_d9a21354bcb648e4a7d7549d8c4a31ae"}},"f489a1d89ee24e3d9e4cdb8ab11d18ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6241b90c539417ab85ff6d7e079f0ee","placeholder":"​","style":"IPY_MODEL_c7d5eda9338546a292f7456d7d9a87cd","value":"Generating dev split: "}},"d1cf3cb3f4b847889222dcc3104eaa3c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6abc01d93d4146ec8af1b71a642289c7","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_85fc44e76df942a59c76a65703439b92","value":1}},"8d6a43d53f384e32bcc99aea31b92e79":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_06928d9c9fc7437897a5d8150579787d","placeholder":"​","style":"IPY_MODEL_8573b9a9e872493e9c5ebba9bc3fe4cd","value":" 1743/0 [00:00&lt;00:00, 2909.33 examples/s]"}},"d9a21354bcb648e4a7d7549d8c4a31ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6241b90c539417ab85ff6d7e079f0ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7d5eda9338546a292f7456d7d9a87cd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6abc01d93d4146ec8af1b71a642289c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"85fc44e76df942a59c76a65703439b92":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"06928d9c9fc7437897a5d8150579787d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8573b9a9e872493e9c5ebba9bc3fe4cd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"76cca57478c14d8a90b6c40723ae359b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_42d829723d594b12bdf830b4eb376c7e","IPY_MODEL_d40cf66093944fbc942e4ad8b71cc277","IPY_MODEL_81f08e03061343a09de81fe6123b2aa1"],"layout":"IPY_MODEL_952cc348fce6475f9a88353dcfbb5330"}},"42d829723d594b12bdf830b4eb376c7e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd912f5ae6ca4c88b400e5f2a7926c8b","placeholder":"​","style":"IPY_MODEL_67aab0371950497a96e762681f38b92e","value":"tokenizer_config.json: 100%"}},"d40cf66093944fbc942e4ad8b71cc277":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0927fb67189473e9f40cc1db7eb14db","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6669197d664647c3ad46f8607c997fe3","value":48}},"81f08e03061343a09de81fe6123b2aa1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ab228aefcdc46ada3b6243fa1ec32c9","placeholder":"​","style":"IPY_MODEL_e6948cbac63c43888659f7f8848249b9","value":" 48.0/48.0 [00:00&lt;00:00, 2.81kB/s]"}},"952cc348fce6475f9a88353dcfbb5330":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd912f5ae6ca4c88b400e5f2a7926c8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67aab0371950497a96e762681f38b92e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e0927fb67189473e9f40cc1db7eb14db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6669197d664647c3ad46f8607c997fe3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2ab228aefcdc46ada3b6243fa1ec32c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6948cbac63c43888659f7f8848249b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae05fd298e3c4d958e5223e93d44c831":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c2ae7c5a4520430bbbfa25afdb41426f","IPY_MODEL_a3b9967f3cd04b2ba9ab04580b056971","IPY_MODEL_fd1a249fb5d84ddcbe44e0f6958e900c"],"layout":"IPY_MODEL_f780ca0ec2a0441d9b34abf62c253cf3"}},"c2ae7c5a4520430bbbfa25afdb41426f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_360c083a6d1448319bf1aeea72008a84","placeholder":"​","style":"IPY_MODEL_61e55af82ddf46389c8f705429642f49","value":"vocab.txt: 100%"}},"a3b9967f3cd04b2ba9ab04580b056971":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f0d80fce2f1e47e7af60aa4921243895","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7286a20db44f4ff68d5f22a5a0d058e9","value":231508}},"fd1a249fb5d84ddcbe44e0f6958e900c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_619472f3ce5a474e8765c05a40978f2d","placeholder":"​","style":"IPY_MODEL_460b4d25084f45aeb347f79272043f9a","value":" 232k/232k [00:00&lt;00:00, 6.91MB/s]"}},"f780ca0ec2a0441d9b34abf62c253cf3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"360c083a6d1448319bf1aeea72008a84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61e55af82ddf46389c8f705429642f49":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f0d80fce2f1e47e7af60aa4921243895":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7286a20db44f4ff68d5f22a5a0d058e9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"619472f3ce5a474e8765c05a40978f2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"460b4d25084f45aeb347f79272043f9a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"64c9a20dd1164e579b9e74cb684cf3b7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_54a33275ad9b4625b2ed4602cef8aa8d","IPY_MODEL_cf8799146b1548a9b182ab0e3c042eb5","IPY_MODEL_bcd9300ed26c4558a5f97f65c22f25c8"],"layout":"IPY_MODEL_02e03efd0e2346698fa677e1a74eadb3"}},"54a33275ad9b4625b2ed4602cef8aa8d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_48636506315b4de4a84acd903375d87f","placeholder":"​","style":"IPY_MODEL_e2e1a7c110e64e7595a61f96d78a1696","value":"tokenizer.json: 100%"}},"cf8799146b1548a9b182ab0e3c042eb5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a33a76534bf64379a9899c07fa6ff251","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4ac31993c4404cce9745a91e6395975e","value":466062}},"bcd9300ed26c4558a5f97f65c22f25c8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c75b1ae635ed4075a8055d96d7a43877","placeholder":"​","style":"IPY_MODEL_d9731883bed14743aed5334ef431ca43","value":" 466k/466k [00:00&lt;00:00, 5.49MB/s]"}},"02e03efd0e2346698fa677e1a74eadb3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48636506315b4de4a84acd903375d87f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2e1a7c110e64e7595a61f96d78a1696":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a33a76534bf64379a9899c07fa6ff251":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ac31993c4404cce9745a91e6395975e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c75b1ae635ed4075a8055d96d7a43877":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9731883bed14743aed5334ef431ca43":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f61ab886bc6438c9fffa50be7e6e112":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a2478702b2864bea9dc18d8289bd7545","IPY_MODEL_22e4787b59be4293a1c28e5850ed8ced","IPY_MODEL_97a0942bc08b445b8a17b5d11895db19"],"layout":"IPY_MODEL_d34f2709cadc41bba263584162bd5da2"}},"a2478702b2864bea9dc18d8289bd7545":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d78deab68ccf4330b63d611b2ca8cf4f","placeholder":"​","style":"IPY_MODEL_5923f1adda754bdf8f4c3764d86536e1","value":"config.json: 100%"}},"22e4787b59be4293a1c28e5850ed8ced":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd34676771a448bba247717556e01d87","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_682413848696467ab07cdde6c633a9e0","value":570}},"97a0942bc08b445b8a17b5d11895db19":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9ce2dbe40fe4f34a561dfcbd9480094","placeholder":"​","style":"IPY_MODEL_af6eba84155b409aa96afbc858b38c98","value":" 570/570 [00:00&lt;00:00, 20.1kB/s]"}},"d34f2709cadc41bba263584162bd5da2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d78deab68ccf4330b63d611b2ca8cf4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5923f1adda754bdf8f4c3764d86536e1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd34676771a448bba247717556e01d87":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"682413848696467ab07cdde6c633a9e0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a9ce2dbe40fe4f34a561dfcbd9480094":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af6eba84155b409aa96afbc858b38c98":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}